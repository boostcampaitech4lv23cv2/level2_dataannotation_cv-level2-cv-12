{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset\n",
    "\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonttypes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10']\n",
    "\n",
    "SRC_DATASET_DIR = '/opt/ml/input/AIHUB_doc_original'  # 파일 위치\n",
    "DST_DATASET_DIR = '/opt/ml/input/data/AIHub_Docs'  # 선택한 파일 저장 위치\n",
    "\n",
    "NUM_WORKERS = 4  # FIXME\n",
    "\n",
    "IMAGE_EXTENSIONS = {'.gif', '.jpg', '.png'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_mkdir(x):\n",
    "    if not osp.exists(x):\n",
    "        os.makedirs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(image_dir, label_dir):\n",
    "    subdirs = glob(image_dir + '/???')\n",
    "    path_images = []\n",
    "    path_labels = []\n",
    "\n",
    "\n",
    "    for subdir in subdirs:          # for doc type\n",
    "        doctype = subdir[-3:]\n",
    "        for fonttype in fonttypes:  # for font type\n",
    "            files = glob(subdir+'/?????'+fonttype+'????.jpg')\n",
    "            f = random.choice(files)\n",
    "            path_images.append(f)\n",
    "            path_labels.append(osp.join(label_dir, doctype, osp.basename(f).split('.')[0]+'.json'))\n",
    "\n",
    "    return path_images, path_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIHubDocsDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, copy_images_to):\n",
    "        # get paths\n",
    "        image_paths, label_paths = get_paths(osp.join(SRC_DATASET_DIR, 'images'),\n",
    "                                                osp.join(SRC_DATASET_DIR, 'labels'))\n",
    "        assert len(image_paths) == len(label_paths)\n",
    "\n",
    "        sample_ids, samples_info = list(), dict()\n",
    "        for image_path in image_paths:\n",
    "            # find label & image path pair\n",
    "            sample_id = osp.splitext(osp.basename(image_path))[0]    # ('00510012043', '.jpg')\n",
    "            label_path = osp.join(label_dir, sample_id[:3], '{}.json'.format(sample_id))\n",
    "            assert label_path in label_paths\n",
    "\n",
    "            # get word dict\n",
    "            words_info = self.parse_label_file(label_path)\n",
    "            \n",
    "            sample_ids.append(sample_id)\n",
    "            samples_info[sample_id] = dict(image_path=image_path, label_path=label_path,\n",
    "                                           words_info=words_info)\n",
    "            \n",
    "        self.sample_ids, self.samples_info = sample_ids, samples_info\n",
    "\n",
    "        self.copy_images_to = copy_images_to\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_info = self.samples_info[self.sample_ids[idx]]\n",
    "\n",
    "        image_fname = osp.basename(sample_info['image_path'])\n",
    "        image = Image.open(sample_info['image_path'])\n",
    "        img_w, img_h = image.size\n",
    "\n",
    "        if self.copy_images_to:\n",
    "            maybe_mkdir(self.copy_images_to)\n",
    "            image.save(osp.join(self.copy_images_to, osp.basename(sample_info['image_path'])))\n",
    "\n",
    "        license_tag = dict()\n",
    "        sample_info_ufo = dict(img_h=img_h, img_w=img_w, words=sample_info['words_info'], tags=[\"document\"],\n",
    "                               license_tag=license_tag)\n",
    "\n",
    "        return image_fname, sample_info_ufo\n",
    "\n",
    "    def parse_label_file(self, label_path):\n",
    "        re_ko = re.compile('[ㄱ-ㅣ가-힣]')\n",
    "        re_en = re.compile('[a-zA-Z]')\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            label = json.load(f)\n",
    "\n",
    "        words_info = dict()  #words_info, languages = dict(), set()\n",
    "        for i in range(len(label['text']['word'])):\n",
    "            label_word = label['text']['word'][i]\n",
    "            # points\n",
    "            x_ul, y_ul, x_lr, y_lr = label_word['wordbox']\n",
    "            points = [[x_ul, y_ul], [x_lr, y_ul], [x_lr, y_lr], [x_ul, y_lr]]\n",
    "            # transcription\n",
    "            transcription = label_word['value']\n",
    "            # language\n",
    "            languagelist = []\n",
    "            if re_ko.search(transcription):\n",
    "                languagelist.append('ko')\n",
    "            if re_en.search(transcription):\n",
    "                languagelist.append('en')\n",
    "            # illegibility\n",
    "            illegibility = False\n",
    "            # orientation\n",
    "            orientation = 'Horizontal'     # 정보가 없어서 임의로 넣음. 실제로는 vertical 존재\n",
    "            # word_tags\n",
    "            word_tags=None\n",
    "            words_info[str(i)] = dict(\n",
    "               points=points, transcription=transcription, language=languagelist,\n",
    "                illegibility=illegibility, orientation=orientation, word_tags=word_tags\n",
    "            )\n",
    "        return words_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:12<00:00, 40.59it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = AIHubDocsDataset(osp.join(SRC_DATASET_DIR, 'images'),\n",
    "                            osp.join(SRC_DATASET_DIR, 'labels'),\n",
    "                            copy_images_to=osp.join(DST_DATASET_DIR, 'images'))\n",
    "\n",
    "anno = dict(images=dict())\n",
    "with tqdm(total=len(dataset)) as pbar:\n",
    "    for batch in DataLoader(dataset, num_workers=NUM_WORKERS, collate_fn=lambda x: x):\n",
    "        image_fname, sample_info = batch[0]\n",
    "        anno['images'][image_fname] = sample_info\n",
    "        pbar.update(1)\n",
    "\n",
    "ufo_dir = osp.join(DST_DATASET_DIR, 'ufo')\n",
    "maybe_mkdir(ufo_dir)\n",
    "with open(osp.join(ufo_dir, 'AIHub_Docs.json'), 'w') as f:\n",
    "    json.dump(anno, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
